## 1. **선형 회귀 (Linear Regression)**

**정의**: 출력 y는 입력 x에 대해 y=wx+b 형태로 예측

**목적**: 입력 변수와 출력 변수 간의 선형 관계를 모델링

**핵심 개념**: 최소 제곱법(Least Squares)을 통해 직선(또는 초평면)을 찾음

**장점**: 단순하고 해석이 쉬움, 계산 속도가 빠름

**단점**: 선형 가정이 어긋난 문제에서는 성능 저하

**사용 사례**: 주택 가격 예측, 온도 예측 등


## 2. **로지스틱 회귀 (Logistic Regression)**

**정의**: 입력 x에 대해 시그모이드 함수를 사용해 확률로 변환

**목적**: 이진 또는 다중 범주 분류

**핵심 개념**: 최대우도추정(MLE) 기반으로 모델 파라미터 학습

**장점**: 확률 기반 출력, 해석 용이

**단점**: 비선형 분포에서는 성능 저하

**사용 사례**: 스팸 메일 필터링, 고객 이탈 예측


## 3. **결정 트리 (Decision Tree)**

**정의**:입력 특징(feature)에 따라 분기하며 예측값에 도달

**목적**: 데이터를 여러 조건에 따라 분기하여 예측

**핵심 개념**: 지니 불순도(Gini Impurity), 엔트로피(Entropy) 등을 기준으로 분기

**장점**: 해석 가능, 범주형/수치형 데이터 모두 사용 가능

**단점**: 과적합(overfitting) 우려

**사용 사례**: 의료 진단, 고객 분류

## 4. **랜덤 포레스트 (Random Forest)**

**정의**: 여러 결정 트리를 랜덤하게 만들어 평균/투표 방식으로 예측

**목적**: 여러 개의 결정 트리를 조합해 예측 성능 향상

**핵심 개념**: 배깅(Bagging) + 무작위 특성 선택

**장점**: 과적합 감소, 견고한 예측

**단점**: 느린 예측 속도, 해석 어려움

**사용 사례**: 금융 사기 탐지, 품질 분류

## 5. **K-최근접 이웃 (K-Nearest Neighbors, KNN)**

**정의**: 예측하려는 포인트에서 가장 가까운 k개의 이웃의 값을 기반으로 결정

**목적**: 거리 기반으로 주변 데이터를 참조하여 예측

**핵심 개념**: 유클리드 거리 또는 코사인 거리 등으로 이웃 측정

**장점**: 훈련 필요 없음, 직관적

**단점**: 계산량 많음, 고차원에서 성능 저하

**사용 사례**: 이미지 분류, 추천 시스템

## 6. **SVM (Support Vector Machine)**

**목적**: 마진이 최대가 되는 결정 경계를 찾는 분류 알고리즘

**정의**: 클래스 간 간격(margin)을 최대화하는 초평면(hyperplane) 생성

**핵심 개념**: 커널 트릭(kernel trick)을 이용해 비선형 분류 가능

**장점**: 작은 데이터셋에서 강력함, 고차원 처리 가능

**단점**: 대용량 데이터에서 느림, 하이퍼파라미터 민감

**사용 사례**: 문자 인식, 유전자 데이터 분류

## 7. **인공 신경망 (Artificial Neural Network, ANN)**

**목적**: 인간 뇌의 구조를 모방해 복잡한 비선형 관계 학습

**정의**:여러 개의 뉴런이 계층적으로 연결된 구조를 통해 입력 → 출력 변환

**핵심 개념**: 역전파(Backpropagation) 알고리즘으로 가중치 업데이트

**장점**: 강력한 표현력, 비선형 문제 해결 가능

**단점**: 많은 데이터와 계산 자원 필요, 과적합 위험

**사용 사례**: 음성 인식, 이미지 분류, 자율주행

## 8. **Gradient Boosting (e.g. XGBoost, LightGBM)**

**목적**: 약한 모델들을 순차적으로 조합해 성능 향상

**정의**:이전 모델의 오차를 줄이는 방향으로 새로운 모델을 추가

**핵심 개념**: 손실 함수의 기울기를 따라 모델을 갱신

**장점**: 높은 성능, 강건함

**단점**: 훈련 속도 느림, 과적합 가능성 있음

**사용 사례**: Kaggle 대회, 금융 예측




















